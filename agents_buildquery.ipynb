{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aqSlMc27noo"
      },
      "outputs": [],
      "source": [
        "!pip install -qU openai-agents==0.0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBGHkh7nBVHp"
      },
      "source": [
        "First let's set our [OpenAI API key](https://platform.openai.com/settings/organization/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwpk_H5_BnAl",
        "outputId": "89f732f4-db60-4b68-e937-f95ec84cdaeb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \\\n",
        "  getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lqkaleTAAjHK"
      },
      "outputs": [],
      "source": [
        "from agents import Agent, Runner\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You're a helpful assistant\",\n",
        "    model=\"gpt-4o-mini\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "Bjn-gyR9BIn-",
        "outputId": "4d2f3062-7d78-4dae-8b1e-ecfebb594ba7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Once upon a time in a small village nestled between lush green hills, there lived a young girl named Lila. Lila had an extraordinary gift: she could speak to animals. While other children played with toys, Lila wandered the fields and forests, chatting with birds, rabbits, and even the wise old owl who perched on the ancient oak tree.\\n\\nOne sunny afternoon, while exploring the edge of the village, Lila heard a faint whimpering sound. Following the noise, she found a tiny fox trapped in a thicket of thorny bushes. The little creature's bright eyes sparkled with fear, and Lila's heart went out to it.\\n\\n“Don’t worry, I’m here to help,” she whispered gently. The fox looked at her, seemingly understanding, and calmed down as she carefully freed it from the thorns. Once free, the fox nuzzled Lila’s hand, grateful for her kindness.\\n\\nIn return, the fox led Lila to a hidden glade filled with shimmering flowers that glowed in the soft afternoon light. “This is the Secret Garden,” the fox whispered, “a place where dreams come alive.”\\n\\nIntrigued, Lila visited the glade each day, discovering that the flowers held magical properties: they could grant wishes. Lila used her wishes wisely—helping her village with bountiful harvests and healing the sick. \\n\\nAs the seasons changed, Lila learned that true magic didn’t come from wishes alone but from the bonds she formed with her friends—both human and animal. The village flourished not just because of her magic, but because everyone came together in gratitude and kindness.\\n\\nLila never forgot the little fox, who became her faithful companion. Together, they shared laughter and adventures, teaching the villagers the value of harmony with nature.\\n\\nAnd so, the secret of the glade lived on, not just in wishes, but in the hearts of those who learned to listen—to each other and the world around them. The village thrived, reminded that true magic exists in kindness and friendship.\""
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"tell me a short story\"\n",
        ")\n",
        "result.final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBnhUepDBPHM",
        "outputId": "043f936a-7aea-4ec8-fdfd-a7aeb9ea2f39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None, max_tokens=None), tools=[], mcp_servers=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='resp_67ee87ae7db48192bbe3a039b81ea49a04fc9325808c570b', created_at=1743685550.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=False, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseInProgressEvent(response=Response(id='resp_67ee87ae7db48192bbe3a039b81ea49a04fc9325808c570b', created_at=1743685550.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=False, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.in_progress'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_67ee87aedcb88192bb00ef3767f7f25a04fc9325808c570b', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='msg_67ee87aedcb88192bb00ef3767f7f25a04fc9325808c570b', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='Hello', item_id='msg_67ee87aedcb88192bb00ef3767f7f25a04fc9325808c570b', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='!', item_id='msg_67ee87aedcb88192bb00ef3767f7f25a04fc9325808c570b', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' How', item_id='msg_67ee87aedcb88192bb00ef3767f7f25a04fc9325808c570b', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' can', item_id='msg_67ee87aedcb88192bb00ef3767f7f25a04fc9325808c570b', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' I', item_id='msg_67ee87aedcb88192bb00ef3767f7f25a04fc9325808c570b', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' assist', item_id='msg_67ee87aedcb88192bb00ef3767f7f25a04fc9325808c570b', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' you', item_id='msg_67ee87aedcb88192bb00ef3767f7f25a04fc9325808c570b', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' today', item_id='msg_67ee87aedcb88192bb00ef3767f7f25a04fc9325808c570b', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='?', item_id='msg_67ee87aedcb88192bb00ef3767f7f25a04fc9325808c570b', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDoneEvent(content_index=0, item_id='msg_67ee87aedcb88192bb00ef3767f7f25a04fc9325808c570b', output_index=0, text='Hello! How can I assist you today?', type='response.output_text.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='msg_67ee87aedcb88192bb00ef3767f7f25a04fc9325808c570b', output_index=0, part=ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text'), type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_67ee87aedcb88192bb00ef3767f7f25a04fc9325808c570b', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='resp_67ee87ae7db48192bbe3a039b81ea49a04fc9325808c570b', created_at=1743685550.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_67ee87aedcb88192bb00ef3767f7f25a04fc9325808c570b', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=17, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=10, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=27), user=None, store=True), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None, max_tokens=None), tools=[], mcp_servers=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='msg_67ee87aedcb88192bb00ef3767f7f25a04fc9325808c570b', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ],
      "source": [
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"hello there\"\n",
        ")\n",
        "async for event in response.stream_events():\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH-RoQfzMj6T"
      },
      "source": [
        "We can filter these various event types to find only raw tokens like so:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Iixlv0sRMqr"
      },
      "source": [
        "## Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDHORYUcROZn"
      },
      "source": [
        "OpenAI included **function tools** as a key feature in their Agents SDK announcement. After turning everyone away from using _function calling_ to instead use _tool calling_, OpenAI have now decided that an LLM deciding to execute some code will be called _\"function tools\"_.\n",
        "\n",
        "To use _function tools_ in Agents SDK we simply decorate a function with the `@function_tool` decorator like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tJz7BlwiNOa3"
      },
      "outputs": [],
      "source": [
        "from agents import function_tool\n",
        "from typing import List, Union, Optional, Tuple\n",
        "\n",
        "# @function_tool\n",
        "# def multiply(x: float, y: float) -> float:\n",
        "#     \"\"\"Multiplies `x` and `y` to provide a precise\n",
        "#     answer.\"\"\"\n",
        "#     return x*y\n",
        "\n",
        "\n",
        "from typing import List, Optional, Union, Tuple\n",
        "\n",
        "from typing import List, Optional, Union, Tuple, Dict\n",
        "\n",
        "@function_tool\n",
        "def build_query(\n",
        "    select_columns: List[str],\n",
        "    table_name: str,\n",
        "    where_conditions: Optional[Union[\n",
        "        Dict[str, Union[str, int, float]],\n",
        "        List[List[Union[str, int, float]]]  # Changed from Tuple[...] to List[...]\n",
        "    ]] = None,\n",
        "    group_by_columns: Optional[List[str]] = None,\n",
        "    having_conditions: Optional[Union[\n",
        "        Dict[str, Union[str, int, float]],\n",
        "        List[List[Union[str, int, float]]]  # Changed from Tuple[...] to List[...]\n",
        "    ]] = None\n",
        ") -> str:\n",
        "    query = f\"SELECT {', '.join(select_columns)} FROM {table_name}\"\n",
        "    \n",
        "    # Process WHERE conditions\n",
        "    if where_conditions:\n",
        "        if isinstance(where_conditions, dict):\n",
        "            conditions = \" AND \".join(\n",
        "                f\"{col} = '{val}'\" if isinstance(val, str) else f\"{col} = {val}\"\n",
        "                for col, val in where_conditions.items()\n",
        "            )\n",
        "        else:  # List of tuples\n",
        "            conditions = \" AND \".join(\n",
        "                f\"{col} {op} '{val}'\" if isinstance(val, str) else f\"{col} {op} {val}\"\n",
        "                for col, op, val in where_conditions\n",
        "            )\n",
        "        query += f\" WHERE {conditions}\"\n",
        "    \n",
        "    # Process GROUP BY and HAVING conditions\n",
        "    if group_by_columns:\n",
        "        query += f\" GROUP BY {', '.join(group_by_columns)}\"\n",
        "        if having_conditions:\n",
        "            if isinstance(having_conditions, dict):\n",
        "                conditions = \" AND \".join(\n",
        "                    f\"{col} = '{val}'\" if isinstance(val, str) else f\"{col} = {val}\"\n",
        "                    for col, val in having_conditions.items()\n",
        "                )\n",
        "            else:\n",
        "                conditions = \" AND \".join(\n",
        "                    f\"{col} {op} '{val}'\" if isinstance(val, str) else f\"{col} {op} {val}\"\n",
        "                    for col, op, val in having_conditions\n",
        "                )\n",
        "            query += f\" HAVING {conditions}\"\n",
        "    \n",
        "    return query\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8b8KektTQbK"
      },
      "source": [
        "Note that we have taken extra care to include a clear and descriptive function name, relatively clear parameter names, type annotations for both input parameters and expected output, and a natural language docstring that will be fed to the LLM and explain to it _what_ this tool does.\n",
        "\n",
        "To run our agent _with_ tools we simply pass our new tool into the `tools` parameter during `Agent` initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WnT4C9WBTO_M"
      },
      "outputs": [],
      "source": [
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=(\n",
        "        \"You're an experienced data analyse \"\n",
        "        \"use the provided tools whenever possible. Do not \"\n",
        "        \"rely on your own knowledge too much and instead \"\n",
        "        \"use your tools to help you answer queries.\",\n",
        "        \"I AM PROVIDING you with tool to retrive the data using retrieve relevent data IF REQUIRED this function\"\n",
        "        \"contains parameters:-(select_columns, table_name, where_conditions=None, group_by_columns=None, having_conditions=None)\"\n",
        "        \"NOTE THAT I WILL BE PROVIDING YOU THE DETAILS AND USE THIS FUNCTION  AND ADJUST THE PARAMETERS RESPECTIVELY BASED ON MY AS TO RETRIEVE THE DATA AND GIVE ME THE ANSWER\"\n",
        "    ),\n",
        "    model=\"gpt-4o\",\n",
        "    tools=[build_query]  # note that we expect a list of tools\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtioNxflUS-s"
      },
      "source": [
        "Now let's initialize a new runner and execute our agent with tools:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSOREQ-XUSVi",
        "outputId": "579e3dc2-832c-4d35-da30-5bcbd757248b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Instructions must be a string or a function, got (\"You're an experienced data analyse use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", 'I AM PROVIDING you with tool to retrive the data using retrieve relevent data IF REQUIRED this functioncontains parameters:-(select_columns, table_name, where_conditions=None, group_by_columns=None, having_conditions=None)NOTE THAT I WILL BE PROVIDING YOU THE DETAILS AND USE THIS FUNCTION  AND ADJUST THE PARAMETERS RESPECTIVELY BASED ON MY AS TO RETRIEVE THE DATA AND GIVE ME THE ANSWER')\n",
            "Instructions must be a string or a function, got (\"You're an experienced data analyse use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", 'I AM PROVIDING you with tool to retrive the data using retrieve relevent data IF REQUIRED this functioncontains parameters:-(select_columns, table_name, where_conditions=None, group_by_columns=None, having_conditions=None)NOTE THAT I WILL BE PROVIDING YOU THE DETAILS AND USE THIS FUNCTION  AND ADJUST THE PARAMETERS RESPECTIVELY BASED ON MY AS TO RETRIEVE THE DATA AND GIVE ME THE ANSWER')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To determine if the issue with the execution order was due to the `custom_input` column being an empty list for `step_id` 25, I used a query to retrieve the `custom_input` for that specific step:\n",
            "\n",
            "```sql\n",
            "SELECT step_id, custom_input FROM steps WHERE step_id = 25;\n",
            "```\n",
            "\n",
            "You can run this query on your database to check the value of `custom_input`. If it is an empty list, it confirms the condition described."
          ]
        }
      ],
      "source": [
        "\n",
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\" In table steps the step_id 22 was supposed to be executed before step_id 32, but the opposite happened this case could only happen when \" \\\n",
        "    \"custom_input column of step_id 25 is an empty list [], how do i figure this out using steps table how do i view that table also mention if you used the tool to determine or not\"\n",
        ")\n",
        "        \n",
        "async for event in response.stream_events():\n",
        "    if event.type == \"raw_response_event\" and \\\n",
        "        isinstance(event.data, ResponseTextDeltaEvent):\n",
        "        print(event.data.delta, end=\"\", flush=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
